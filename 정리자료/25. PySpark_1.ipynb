{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark는 large-scale의 데이터 프로세싱을 위한 통합된 분석 엔진이다.  \n",
    "  \n",
    "분산된 여러 대의 노드에서 연산을 할 수 있게 해주는 범용 분산 클러스터링 플랫폼.  \n",
    "Memery Hadoop이라고도 불리는데, 기존의 하둡이 Map & Reduce 작업을 디스크 기반으로 하기 때문에 느려지는 성능을 메모리 기반으로 옮겨서 고속화 하고자 하는데서 출발함. \n",
    "\n",
    "['조대협의 블로그 : Apache Spark'에 대한 설명](http://bcho.tistory.com/1023?category=563141)\n",
    "\n",
    "Spark program은 일반적으로 \"Driver Program\"이라고 하는데, 이러한 driver program은 여러개의 병렬적인 작업으로 나눠져서 Spark의 Worker Node(서버)에 있는 Executor(프로세스)에서 실행된다.  \n",
    "\n",
    "1. SparkContext가 SparkClusterManager에 접속한다. 이 클러스터 메니져는 스팍 자체의 클러스터 메니져가 될 수 도 있고 Mesos,YARN 등이 될 수 있다. 이 클러스터 메니저를 통해서 가용한 Excutor 들을 할당 받는다\n",
    "2. Excutor를 할당 받으면, 각각의 Executor들에게 수행할 코드를 보낸다.\n",
    "3. 다음으로 각 Excutor 안에서 Task에서 로직을 수행한다.\n",
    "\n",
    "\n",
    "출처: http://bcho.tistory.com/1025?category=563141 [조대협의 블로그]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "# pyspark를 import할 수 있도록 찾아주는 라이브러리이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "findspark.init() # pyspark를 일반적인 라이브러리처럼 import할 수 있게 해줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# Spark 기능에 대한 기본 entry-point(진입점).\n",
    "# SparkContext는 Spark Cluster에 대한 연결을 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate() # default spark context 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop() \n",
    "# sparkContext를 여러개 만들면 안돌아간다. 그래서 새로운 context생성시 먼저 stop을 해줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(master = 'local[2]', appName = 'Spark Test')\n",
    "# master: 연결할 Cluster URL(e.g. mesos://host:port, spark://host:port, local[4]).\n",
    "# appName: cluster web UI에 띄울 작업의 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://taeuk:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Spark Test>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rdd = sc.parallelize([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RDD__ (__R__esilient __D__istributed __D__ataset)  \n",
    "RDD는 여러 분산 노드에 걸쳐서 저장되는 변경 불가능한 데이터(객체)의 집합이며,  \n",
    "각각의 RDD는 여러개의 파티션으로 분리되어 서로 다른 노드에서 실행된다.  \n",
    "쉽게 말해서 스파크 내에 저장된 데이터를 의미한다.  \n",
    "\n",
    "RDD는 변경이 불가하기 때문에 변경하기 위해선 새로 다시 생성해야 한다.    \n",
    "외부로부터 데이터를 로딩하거나 코드에서 생성된 데이터를 저장함으로써 생성 가능.  \n",
    "  \n",
    "RDD에서는 2가지의 operation만 지원해준다.\n",
    " - Transformation(변환) : 기존의 RDD 데이타를 변경하여 새로운 RDD 데이타를 생성해내는 것. 흔한 케이스는 filter와 같이 특정 데이타만 뽑아 내거나 map 함수 처럼, 데이타를 분산 배치 하는 것 등을 들 수 있다.\n",
    " - Action(액션) : RDD 값을 기반으로 무엇인가를 계산해서(computation) 결과를 생성해 내는것으로 가장 쉬운 예로는 count()와 같은 operation들을 들 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop() # SparkContext를 중지시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "# SparkConf : Configuration for Spark App. Spark 어플을 설정하는데 쓰인다.\n",
    "# 다양한 Spark 파라미터들을 key-value 쌍으로 세팅하는데 쓰인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "# spark.*로 부터 value들, Java system property들도 또한 로딩해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x1b51dd20080>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.setMaster(\"local[2]\").setAppName(\"Conf\")\n",
    "# 연결할 marster URL을 \"local[2]\", AppName을 \"Conf\"로 지정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "# conf : A L{SparkConf} object setting Spark properties.\n",
    "# conf에 세팅된 Spark property를 SparkContext에 설정해서 객체 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5]) # rdd 생성\n",
    "# 로컬 파이썬 컬렉션을 RDD를 생성하기 위해 분산시킨다.\n",
    "# 만약 input이 성능에 대한 range를 표현한다면 xrange를 사용하는 것을 추천함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:184"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect() # 기본적인 rdd를 생성한 것이다.\n",
    "# collect() : RDD내에 있는 모든 element들을 포함하는 리스트를 리턴해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4, 5]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect() # partition단위로 작업을 한 것이다.\n",
    "# glom() : 모든 element들을 각각의 파티션에 맞게 리스트로 합쳐서 만들어진 RDD 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rdd.glom().collect()) \n",
    "# partition을 2개로 만들어버림. 나중에 파일 2개로 반환해줄것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5], 3) # partition을 3개로 나눠서 rdd 객체 만들어줌\n",
    "# 기본적으로 partition 2개로 만들어줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = sc.parallelize(open(\"./sample/text-test.txt\", encoding='utf-8').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['h', 'e', 'l', 'l', 'o'], ['\\n', 't', 'h', 'i', 's']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.glom().collect() # partition을 나눠서 한 글자씩 받아옴.\n",
    "#glom은 각 partition마다 element들을 나눠서 리턴해준다. = transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'e', 'l', 'l', 'o', '\\n', 't', 'h', 'i', 's']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.collect() # 한 글자씩 다 받아옴.\n",
    "#collect는 모든 element들을 다 받아오는 것이다. = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([\"b\", \"a\", \"c\"], 3) \n",
    "# partition 3개로 나눠서 해당 리스트 rdd 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rddMap = rdd.map(lambda x:(x,1)) # mapping해줌.\n",
    "# 특정 형태로 출력해줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('a', 1)], [('b', 1)], [('c', 1)]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(rddMap.glom().collect()) \n",
    "# partition 3개로 나뉘어져 있는 mapping결과를 정렬해서 출력해줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd.filter(lambda x:x%2==0).collect() # rdd객체에서 filtering한 객체를 출력해줌.\n",
    "# 나누기 해서 나머지가 0이 나오는 것들만 보여줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([2,3,4])\n",
    "rdd = rdd.map(lambda x:range(1, x)) # lambda식으로 mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[range(1, 2)], [range(1, 3), range(1, 4)]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect() # 기본적으로 partition2개로 나눠진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 1, 2, 3]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap의 경우 각 RDD에 적용시 partition단위로 출력하며, 없는 데이터는 삭제해준다.\n",
    "\n",
    "rdd = sc.parallelize([2,3,4])\n",
    "rdd.flatMap(lambda x:range(1,x)).collect()\n",
    "\n",
    "# flatMap의 경우 Mapping해준뒤 1열로 하나의 리스트에 한개씩 쭉 나열해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([\"Roses are red\", \"Violets are blue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Roses', 'are', 'red'], ['Violets', 'are', 'blue']]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x:x.split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Roses', 'are', 'red', 'Violets', 'are', 'blue']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([\"Roses are red\", \"Violets are blue\"])\n",
    "rdd.flatMap(lambda x:x.split()).collect()\n",
    "# flatMap의 경우 Mapping해준뒤 1열로 하나의 리스트에 한개씩 쭉 나열해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__map vs flatMap__  \n",
    "\n",
    "__map__ : RDD의 각 요소에 주어진 함수를 적용하여 새로운 RDD를 반환합니다. map 함수는 하나의 항목 만 반환합니다.  \n",
    "  \n",
    "__flatMap__ : map 와 마찬가지로 RDD의 각 요소에 함수를 적용하여 새로운 RDD를 반환하지만 출력은 병합됩니다.  \n",
    "\n",
    "\n",
    "map과 flatMap은 비슷하다. 즉 입력 RDD로부터 한 줄을 가져 와서 그 위에 함수를 적용한다. 이들이 다른 점은 map의 함수는 하나의 요소 만 반환하는 반면 flatMap의 함수는 요소 목록 (0 이상)을 반복자로 반환 할 수 있다는 점입니다.  \n",
    "  \n",
    "또한 flatMap의 출력이 병합됩니다. flatMap의 함수가 요소 목록을 반환하더라도 flatMap은 목록의 모든 요소가 플랫이 아닌 (목록이 아닌) RDD를 반환합니다.  \n",
    "\n",
    "[map과 flatMap의 차이점 참고 링크](https://code.i-harness.com/ko-kr/q/1550b82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,1,2,3]).distinct().collect()\n",
    "#distinct()는 중복된 결과를 없애준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(100), 4) # partition을 4개로 줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sample은 일부 샘플만 추출해서 가져온다. RDD의 subset을 추출해줌.\n",
    "subset = rdd.sample(False, 0.1, 12)\n",
    "# 중복 False, 10% 비율, seed =12로 주고 랜덤으로 샘플링 rdd 객체 생성\n",
    "# 다만 비율이 정확한 값이 아니라 대략 10%를 갖고 오는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[35] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 10, 15, 16, 19], [26, 32], [62, 63, 65, 72], [98]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.glom().collect() # 100개 중에 12개를 갖고 왔음을 알 수 있다.\n",
    "# 비율이 대략적인 값을 나타냄을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.count() # rdd 객체 내부의 element 개수 : action\n",
    "# count()의 경우 spark내부 함수. 그래서 len()보다는 더 최적화되어 빠르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__JOIN__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\",1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\",2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('b', (4, None))]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Left Outer Join\n",
    "sorted(x.leftOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (2, 1))]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(y.leftOuterJoin(x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2))]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Right Outer Join\n",
    "sorted(x.rightOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2))]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join : 키 값이 같은 것들에 대해 join\n",
    "sorted(x.join(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intersection : 교집합해줌.\n",
    "sorted(x.intersection(y).collect()) # 완전히 교집합 되는 것이 없어서 출력되는것이 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 4)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\",1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\",2), (\"b\", 4)])\n",
    "sorted(x.intersection(y).collect()) # (\"b\", 4)가 완전히 교집합 되기 때문에 출력됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__repartition__  \n",
    ": 파티션 개수가 다를 경우 join이나 zip할 때 오류가 발생할 수 있어  \n",
    "그것을 방지하기 위해 다시 partition 개수를 수정해준다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().count() # 현재 4개의 partition을 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84],\n",
       " [20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94],\n",
       " [35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99],\n",
       " [45, 46, 47, 48, 49],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartition(10).glom().collect() #파티션 개수를 10개로 수정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://localhost:4040/ 으로 들어갈 경우 spark에서 작업을 어떻게 수행했는지 볼수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__take__ : 하나의 partition에서 상위 몇가지의 element 리턴 = transformation  \n",
    "__cache__: 따로 작업공간을 만들어줌. 차후에 꼭!! cache공간을 없애주어야함.  \n",
    "그렇지 않는다면 프로세스 종료후에도 남아있는 경우가 있다.  \n",
    "__takeSample__ : 특정샘플 몇개만 갖고 오는 것. = action  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd_cache = rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[34] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26, 14, 70, 99, 60, 78, 80, 13, 33, 43]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeSample(False, 10, 123) # action 함수이기 때문에 따로 print해줄 필요가 없음.\n",
    "# 중복 False, 10개, seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__reduce__ : key, value 쌍으로 된 것들을 partition 단위로 reduce 시행.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().reduce(lambda x,y:x+y)\n",
    "# [[1, 2] , [3, 4, 5]]\n",
    "# [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#partition 개수에 따라서 생각하는 operation이 아닌 다르게 나올 수 있기 때문에 조심할것.\n",
    "rdd = sc.parallelize([1,2,.5, .1, 5,.2], 3)\n",
    "rdd.reduce(lambda x,y:x/y)\n",
    "\n",
    "# 0.5   ,  5 , 25\n",
    "# 0.01  , 25\n",
    "# 0.004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [0.5, 0.1], [5, 0.2]]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"a\",1), (\"b\",2), (\"a\",3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__reduceByKey__ : 키 값을 기반으로 value값을 더해서 보여줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 4), ('b', 2)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "sorted(rdd.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__foreach__ : RDD 객체 내의 element들을 한개 씩 python형태로 넘겨주는데 쓰임.\n",
    "__foreachPartition__ : RDD객체의 partition을 한개 씩 python형태로 넘겨주는데 쓰인다.\n",
    "__saveAsTextFile__ : RDD객체를 텍스트파일로 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(iterator):\n",
    "    for x in iterator:\n",
    "        print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"a\",1), (\"b\",2), (\"a\",3)], 2)\n",
    "rdd.saveAsTextFile(\"abc\")\n",
    "#위에서 partition을 2개로 나눴기 때문에 2개의 파일에 나뉘어져서 rdd 객체가 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('a', 1)\", \"('b', 2)\", \"('a', 3)\"]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fromText = sc.textFile(\"abc\") #해당 디렉토리 주소를 인자로 주면 해당파일 불러올 수 있다.\n",
    "fromText.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize(\n",
    "    [(\"a\", 22), {\"b\": 23}, [\"c\", 4]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obj = data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj[1][\"b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD는 JVM에서 메모리를 잡고 , python에서는 그냥 통신해서 갖고 오는 것임. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame은 데이터 타입 정의된 테이블 형태로 반환(익숙한 형태의 스키마)해주기 때문에,  \n",
    "row, column에 따라 갖고 올 수 있도록 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame 또한 immutable. 수정할 수 없다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame의 경우 SparkSQL을 쓸 수 있도록 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# pyspark.sql.SparkSession: DataFrame과 SQL 기능을 쓰기 위한 메인 entry-point(진입점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "# 존재하는 SparkSession을 가져오거나, 없다면 새로 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://taeuk:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Conf</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b51de9d6a0>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://taeuk:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Conf</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Conf>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 SparkSession을 만들던 SparkContext를 만들던 어짜피 쓰는 드라이버는 똑같다.  \n",
    "껍데기만 다를 뿐이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
