{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark # pyspark를 import할 수 있도록 해주는 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "findspark.init() # pyspark객체를 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "두개가 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD 와 DataFrame의 차이는 스키마가 있고 없고, 데이터프레임은 raw 데이터를 스키마가 있게 만들어줌. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read.csv, read.format( 포맷 명시 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"sample/ages.csv\")\n",
    "# csv파일을 DataFrame 타입 객체로 반환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'), ('_c1', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes # 모든 컬럼과 타입을 리스트로 출력해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('json').load(\"sample/people.json\")\n",
    "# 'json'포맷의 파일을 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'bigint'), ('name', 'string')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes # 컬럼명이랑 데이터타입을 추론함.\n",
    "\n",
    "#굉장히 명확하게 구조화된 데이터를 불러온다. jdbc 커넥터로 붙일수있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD 였으면 그냥 들어간데로나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"sample/people.json\") # rdd를 가져온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample/people.json MapPartitionsRDD[14] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd) # RDD 타입 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"name\":\"Michael\"}',\n",
       " '{\"name\":\"Andy\", \"age\":30}',\n",
       " '{\"name\":\"Justin\", \"age\":19}']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect() \n",
    "#똑같은파일인데, rdd 로 가져오면 테이블형식이아님.  데이터프레임으로 되면 : 스키마구조를 갖고 , 자동으로 데이터타입을 지정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(rdd) #rdd 로 부터 dataframe 을 만든다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rdd 로 부터 데이터프레임을 만들고 sql 문쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://taeuk:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://taeuk:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ce1b351c50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize((\n",
    "\"\"\"\n",
    "    { \"id\":123,\n",
    "    \"name\":\"Katy\",\n",
    "    \"age\":19,\n",
    "    \"eyeColor\":\"brown\"\n",
    "    }\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "    {\n",
    "    \"id\":124,\n",
    "    \"name\":\"Joe\",\n",
    "    \"age\":44,\n",
    "    \"eyeColor\":\"black\"\n",
    "\n",
    "    }\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "    {\n",
    "    \"id\":125,\n",
    "    \"name\":\"Romanson\",\n",
    "    \"age\":25,\n",
    "    \"eyeColor\":\"blue\"\n",
    "\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    ")) # RDD 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n    { \"id\":123,\\n    \"name\":\"Katy\",\\n    \"age\":19,\\n    \"eyeColor\":\"brown\"\\n    }\\n',\n",
       " '\\n    {\\n    \"id\":124,\\n    \"name\":\"Joe\",\\n    \"age\":44,\\n    \"eyeColor\":\"black\"\\n\\n    }\\n',\n",
       " '\\n    {\\n    \"id\":125,\\n    \"name\":\"Romanson\",\\n    \"age\":25,\\n    \"eyeColor\":\"blue\"\\n\\n    }\\n\\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect() \n",
    "#문자열 그대로 들어감 .처리하려면 regex로 다짤라줘야함 하지만 json 이라는 파일형식을 가지니까. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(rdd) \n",
    "# rdd 에 있는 json 을 그대로가져온다.\n",
    "# 컬럼 , 데이터타입 추론해서 rdd 로 부터 dataframe 을 만든다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, eyeColor: string, id: bigint, name: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df # 추론된 결과가 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pandas 에서 쓰던 DataFrame 과는 달리, 여기의 DataFrame 은 row 단위로 관리한다.   pandas 의 DataFrame은 컬럼으로 관리함.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id=123, name='Katy'),\n",
       " Row(age=44, eyeColor='black', id=124, name='Joe'),\n",
       " Row(age=25, eyeColor='blue', id=125, name='Romanson')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect() # pyspark의 DataFrame은 Row 단위로 관리되는 것을 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#실제 테이블 형태로 가져옴.\n",
    "df.createOrReplaceTempView(\"test\")\n",
    "#view테이블 만들껀데 view table 의 이름을 test 로 지정 dataframe - > temp view 만듬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df에서 collect 같은 기능이 show 다 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+--------+\n",
      "|age|eyeColor| id|    name|\n",
      "+---+--------+---+--------+\n",
      "| 19|   brown|123|    Katy|\n",
      "| 44|   black|124|     Joe|\n",
      "| 25|    blue|125|Romanson|\n",
      "+---+--------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() # 스키마를 가지기때문에 스키마 형태로 데이터를 보여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() # Tree 구조로 Schema를 출력해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, eyeColor: string, id: bigint, name: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from test\") #sql query 날린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id=123, name='Katy'),\n",
       " Row(age=44, eyeColor='black', id=124, name='Joe'),\n",
       " Row(age=25, eyeColor='blue', id=125, name='Romanson')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from test\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD 에서 있던 collect는 Schema없이 출력한 반면에,  \n",
    "여기서는 collect가  row 형태로, 스키마가 있는 형태로 리턴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "새로 RDD 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(\n",
    "    [\n",
    "        (123,\"katie\",19,\"brown\"),\n",
    "        (124,\"joe\",45,\"black\"),\n",
    "        (125,\"romanson\",25,\"blue\")\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(123, 'katie', 19, 'brown'),\n",
       " (124, 'joe', 45, 'black'),\n",
       " (125, 'romanson', 25, 'blue')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 그냥 읽으면 컬럼명 같은것을 다 정해줌 , 하지만 여기서는 우리가 정의할 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *  # 데이터 타입 다 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#name, dataType,nullable,metadata 우리가 정해줄거임.\n",
    "\n",
    "scheme = StructType(\n",
    "    [\n",
    "        StructField(\"id\",LongType(),nullable=True),\n",
    "        StructField(\"name\",StringType(),nullable=True),\n",
    "         StructField(\"age\",LongType(),nullable=True),\n",
    "         StructField(\"eyeColor\",StringType(),nullable=True)\n",
    "    \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema 와 RDD 를 합쳐서 DataFrame 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"test2\") # 해당하는 DataFrame을 임시 View로 만들어줌.\n",
    "# \"test2\"라는 이름의 테이블 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id=123, name='Katy'),\n",
       " Row(age=44, eyeColor='black', id=124, name='Joe'),\n",
       " Row(age=25, eyeColor='blue', id=125, name='Romanson')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이제부터는 sql 작업가능해짐\n",
    "spark.sql(\"select * from test2\").collect() # 해당 sql 쿼리에 대한 답을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*)from test\").show() \n",
    "# sql 쿼리에 대한 결과값을 스키마 형태로 출력해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|123| 19|\n",
      "|124| 44|\n",
      "|125| 25|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id,age from test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|125| 25|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",'age').filter(\"age=25\").show()\n",
    "# age=25로 필터링된 id, age를 출력 \n",
    "# 여기서는 column을 string으로 넣어줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.id, df.age).filter(df.age == 22).show()\n",
    "# age = 22로 필터링해준 id, age를 출력해줌. \n",
    "# 여기서는 column명을 df의 attribute로 넣음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  항공 데이터 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flightPath = \"sample/departuredelays.csv\"\n",
    "airportPath = \"sample/airport-codes-na.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flight = spark.read.csv(flightPath, header = True)\n",
    "# 해당 csv파일을 읽어와서 DataFrame객체로 반환해준다.\n",
    "# header = True 플래그를 통해서 첫줄을 Column명으로 인식해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date='01011245', delay='6', distance='602', origin='ABE', destination='ATL')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight.take(1) # flight DataFrame의 처음Row부터 1개의 Row객체를 갖고 온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airport = spark.read.csv(airportPath, header=True, inferSchema=True, sep=\"\\t\")\n",
    "# header = True를 통해서 첫째 줄을 Column명으로 인식\n",
    "# inferSchema = True를 통해서 input Schema를 자동으로 추론해서 인식해준다.\n",
    "# sep = \"\\t\" 을 통해서 Tab 공백으로 데이터 간을 나눠준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(City='Abbotsford', State='BC', Country='Canada', IATA='YXX')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport.take(1) # 해당 데이터의 Row를 한개 가지고 온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- IATA: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport.printSchema() # 해당 스키마를 트리 구조로서 출력해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight.createOrReplaceTempView(\"flight\")\n",
    "airport.createOrReplaceTempView(\"airport\") # sql문 쓰기 위해 임시로 뷰를 만들어줌.\n",
    "# \"flight\", \"airport\"라는 이름의 테이블 생성됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|   city|origin|   delay|\n",
      "+-------+------+--------+\n",
      "|Seattle|   SEA|159086.0|\n",
      "|Spokane|   GEG| 12404.0|\n",
      "|  Pasco|   PSC|   949.0|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "     select a.city, f.origin,sum(f.delay) as delay from flight f join airport a on a.IATA = f.origin\n",
    "        where a.State = \"WA\"\n",
    "        group by a.City,f.origin\n",
    "        order by delay desc\n",
    "\"\"\").show()\n",
    "\n",
    "#sql문 query를 날릴 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F # SQL문의 built-in 함수."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|   City|origin|sum(delay)|\n",
      "+-------+------+----------+\n",
      "|Seattle|   SEA|  159086.0|\n",
      "|Spokane|   GEG|   12404.0|\n",
      "|  Pasco|   PSC|     949.0|\n",
      "+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#함수형 프로그래밍 형태로 sql문 query날릴 수 있다.\n",
    "\n",
    "airport.join(flight, airport.IATA == flight.origin)\\\n",
    ".where(airport.State == \"WA\")\\\n",
    ".select(airport.City, flight.origin, flight.delay)\\\n",
    ".groupby(airport.City, flight.origin)\\\n",
    ".agg(F.sum(flight.delay))\\\n",
    ".orderBy(\"sum(delay)\", ascending=False)\\\n",
    ".show()\n",
    " # 오름차순 False -> 내림차순 = 큰값부터 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스트리밍(Streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__스트리밍에 대한 이해를 위한 사전 개념__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Unbounded data__ 는 데이터의 수가 정해져있지 않고 계속해서 추가되는, 즉 끊임 없이 흘러 들어오는 데이터라고 볼 수 있다. 예를 들어서 모바일 디바이스에서 계속 올라오는 로그, 페이스북이나 트위터의 타임 피드, 증권 거래 주문 같이 계속 해서 들어와서 쌓이는 데이터를 Unbounded data 라고 한다.   \n",
    "   \n",
    "   \n",
    "__Bounded data__는 데이터가 딱 저장되고 더 이상 증거나 변경이 없는 형태로 계속 유지되는 데이터를 뜻한다. 1월의 정산 데이터. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Event time__ : 데이터의 발생 시간  \n",
    "__Processing time__ : 데이터가 시스템에서 처리되는 시간  \n",
    "    \n",
    "이상적으로는 EventTime과 ProcessingTime이 동일하면 좋겠지만,\n",
    "실제로는 네트워크 상황, 서버CPU, I/O 처리 시간에 따라 ProcessingTime이 더 늦다.\n",
    "\n",
    "ProcessingTime - EventTime의 지연되는 시간을 Skew라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bounded data 처리__ : 그냥 데이터 읽어서 한번에 처리 후 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__UnBounded data 처리__\n",
    "- Batch방식\n",
    " - Fixed Windows : 스트리밍으로 들어오는 데이터를 일정 시간 단위로 모은 뒤에 배치로 처리하는 방식이다.\n",
    " - Sliding Windows : 윈도우(일정 간격 시간 범위)가 움직이면서 데이터를 모으는 개념으로, 윈도우 내에 일정 부분 시간은 서로 겹치게 된다. \n",
    "- Streaming방식 : 기본적으로 Skew(지연시간)이 환경에 따라 들쭉날쭉해서 비교적 복잡함.\n",
    " - Time agnostic : 시간 속성을 지니지 않는 데이터로, 들어오는 대로 처리한다.\n",
    " - Filtering : 특정 데이터만 필터링해서 저장하는 구조.\n",
    " - Inner joins : 2개의 unbounded data에서 서로 비교해서 매칭 뒤에 값을 구하는 방식.\n",
    " - Approximation algorithms : 근사치 추정 방식으로 실시간 분석에서는 전체 데이터를 모두 분석할 시간이 없고, 시급한 분석이 필요하기에 일부만 분석하거나 대략적인 데이터의 근사값을 구하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Windowing__ : 스트리밍 데이터를 처리할 때 일정 시간 간격으로 처리하는 것을 의미  \n",
    "  \n",
    "Fixed Window, Sliding Window방식이 있다.\n",
    "\n",
    "\n",
    "__Session__ : Session Window에는 사용자가 일정 기간동안 반응이 없는 경우에 세션 시작에서 반응이 없어지는 시간 까지를 한 세션으로 묶어서 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__시간대별 Window 처리 방식__\n",
    "\n",
    "Processing Time based Windowing  \n",
    "데이터가 도착한 순서대로 처리해서 저장한다.  \n",
    "  \n",
    "Event Time based Windowing  \n",
    "데이터가 순차적으로 들어온다는 것을 보장할 수 없다.\n",
    "그래서 2가지의 주요 고려 사항이 필요하다.\n",
    " - Buffering\n",
    "   : 늦게 도착한 데이터를 처리해야 하기 때문에. 윈도우를 일정시간동안 유지해야 한다. 이를 위해서 메모리나 별도의 디스크 공간을 사용한다. \n",
    "\n",
    " - Completeness\n",
    "   : Buffering을 적용했으면 다른 문제가 얼마 동안 버퍼를 유지해야 하는가? 즉, 해당 시간에 발생한 모든 데이터는 언제 모두 도착이 완료(Completeness) 되는가? 를 결정하는 것이다. 정확한 완료 시점을 갖는 것은 사실 현실적으로 힘들다. 버퍼를 아주 크게 잡으면 거의 모든 데이타를 잡아낼 수 있겠지만, 버퍼를 아주 크게 잡는 것이 어렵기 때문에, 데이터가 언제 도착할 것이라는 것을 어림 잡아 짐작할 수 있는 방법들이 많다.\n",
    "\n",
    "출처: http://bcho.tistory.com/tag/Sliding%20window [조대협의 블로그]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "# Spark Streaming 기능에 대한 메인 entry-point(진입점)\n",
    "# Streamingontext는 Spark Cluster에 대한 연결을 나타낸다.\n",
    "# DStream을 만드는데 쓰일 수 있다.\n",
    "# Discretized Stream(DStream)은 Spark Streaming에 대한 것이며, \n",
    "# (같은 타입의)RDD들의 연속적인 sequence이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "streaming = StreamingContext(sc, 5)\n",
    "#인자로 sparkContext, batchDuration(배치가 되는 시간 간격)을 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.streaming.context.StreamingContext at 0x1ce1b3be978>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = streaming.socketTextStream(\"localhost\", 9999)\n",
    "#스트리밍으로 받아올 포트를 연결.\n",
    "#rdd이다 보니까 flatMap같은것을 쓸 수 있다.\n",
    "\n",
    "# TCP socket을 이용해서 데이터를 받아온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line:line.split())\n",
    "# DStream객체의 모든 element들에 대해서 해당 함수 인자로 넣어 실행하고,\n",
    "# 1열로 쭉 나열한 상태로 반환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.TransformedDStream"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs = words.map(lambda w:(w,1))\n",
    "# DStream객체의 모든 element들에 대해서 해당 함수 인자로 넣어 실행해서 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.TransformedDStream"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = pairs.reduceByKey(lambda x,y:x+y)\n",
    "# 키값에 대해서 reduce를 각각의 RDD에 적용하고 DStream 객체로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 스트리밍 받는다고 가정하에 디자인하고 있는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:10:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:10:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:10:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:10:15\n",
      "-------------------------------------------\n",
      "('apple', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:10:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:10:25\n",
      "-------------------------------------------\n",
      "('hi', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:10:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:10:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:10:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:10:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:10:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:10:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:11:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:11:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:11:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-10-30 18:11:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# netcat은 서버에다가 request를 보내고, response받는 용도이다. 인터넷 상에 cat을 쓸 수 있다.\n",
    "\n",
    "# 먼저 streaming.start()가 된 이후에\n",
    "# cmd에서 netcat을 실행해서 nc -l -p 9999 를 한뒤 (nc -lvp 9999로 해도 됨.)\n",
    "# 9999port로 apple과 hi를 input을 주었다.\n",
    "\n",
    "streaming.start() # 스트리밍 시작\n",
    "streaming.awaitTermination() # 스트리밍 꺼준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = spark.readStream.format(\"socket\")\\\n",
    ".option(\"host\", \"localhost\")\\\n",
    ".option(\"port\", 9999)\\\n",
    ".load()\n",
    "# localhost:9999 에서 socket을 통해서 데이터를 받아와서 로딩한다.\n",
    "# DataFrame으로 반환받아서 구조적으로 이용할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "# explode : 주어진 array나 map 으로 새로운 row 리턴해줌.\n",
    "# split : reg exp에 따라서 나눠줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "# lines.value에 대해서 공백으로 나눠주고, row를 리턴하며 이름은 \"word\"이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordCount = words.groupby(\"word\").count() # Stream으로 받은 단어의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stream = wordCount.writeStream\\\n",
    ".outputMode(\"complete\")\\ \n",
    ".format(\"console\")\\\n",
    ".start()\n",
    "# console포맷으로 complete(싱크에 맞게 모든 row들 출력)모드로 스트림을 실행\n",
    "\n",
    "stream.awaitTermination() # 스트림 중지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate() \n",
    "# 있던 SparkSession을 불러오거나 새로이 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 144.5, 5.9, 33, \"M\"),\n",
    "        (2, 167.2, 5.4, 45, \"M\"),\n",
    "        (3, 124.1, 5.2, 23, \"F\"),\n",
    "        (1, 144.5, 5.9, 33, \"M\"),\n",
    "        (5, 133.2, 5.7, 54, \"F\"),\n",
    "        (6, 124.1, 5.2, 23, \"F\"),\n",
    "        (7, 129.2, 5.3, 42, \"M\"),    \n",
    "    ],\n",
    "    [\"id\", \"weight\", \"height\", \"age\", \"gender\"]\n",
    ") # 해당 데이터를 DataFrame 객체로 만들어줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 6)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count(), df.distinct().count()\n",
    "# (row의 수 카운트, 중복 제거한 row의 수 카운트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropDuplicates() # 중복된 row를 drop해준다.\n",
    "df.count() # 중복된 row 1개가 제거되고 6개가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  6| 124.1|   5.2| 23|     F|\n",
      "|  7| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"id\").show() # orderBy()를 통해 특정 column기준으로 sorting해줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  6| 124.1|   5.2| 23|     F|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  7| 129.2|   5.3| 42|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  6| 124.1|   5.2| 23|     F|\n",
      "|  7| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates([c for c in df.columns if c != \"id\"]).show()\n",
    "# \"id\" Column을 제외한 컬럼들에서 중복되는 row를 drop해준다.\n",
    "# id= 6, 3이 id를 제외하고는 동일한 값을 지니고 있어 drop됨을 알수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Count|Dcount|\n",
      "+-----+------+\n",
      "|    6|     6|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(\n",
    "    f.count(\"id\").alias(\"Count\"),\n",
    "    f.countDistinct(\"id\").alias(\"Dcount\")). show()\n",
    "# df.agg()는 데이터 전체에 대하여 aggregate 적용할 수 있도록 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+---+------+\n",
      "|           id|weight|height|age|gender|\n",
      "+-------------+------+------+---+------+\n",
      "| 171798691840| 133.2|   5.7| 54|     F|\n",
      "| 481036337152| 144.5|   5.9| 33|     M|\n",
      "| 575525617664| 124.1|   5.2| 23|     F|\n",
      "| 721554505728| 167.2|   5.4| 45|     M|\n",
      "|1099511627776| 129.2|   5.3| 42|     M|\n",
      "|1623497637888| 124.1|   5.2| 23|     F|\n",
      "+-------------+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"id\", f.monotonically_increasing_id()).show()\n",
    "# withColumn은 Column을 추가해주거나 같은 이름의 column을 대체해준다.\n",
    "# f.monotonically_increasing_id()는 증가하는 64bit 정수로 이루어진 column 생성해줌\n",
    "# 그래서 자동으로 id가 겹치지 않도록 해줄 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+---+\n",
      "| id|weight|height|age|gender|old|\n",
      "+---+------+------+---+------+---+\n",
      "|  5| 133.2|   5.7| 54|     F|  5|\n",
      "|  1| 144.5|   5.9| 33|     M|  1|\n",
      "|  6| 124.1|   5.2| 23|     F|  6|\n",
      "|  2| 167.2|   5.4| 45|     M|  2|\n",
      "|  7| 129.2|   5.3| 42|     M|  7|\n",
      "|  3| 124.1|   5.2| 23|     F|  3|\n",
      "+---+------+------+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"old\", df.id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "[\n",
    "    (1, 143.5, 5.6, 28, \"M\", 100000),\n",
    "    (2, 167.2, 5.4, 45, \"M\", None),\n",
    "    (3, None, 5.2, None, None, None),\n",
    "    (4, 144.5, 5.9, 33, \"M\", None),\n",
    "    (5, 133.2, 5.7, 54, \"F\", None),\n",
    "    (6, 124.1, 5.2, None, \"F\", None),\n",
    "    (7, 129.2, 5.3, 42, \"M\", 76000),\n",
    "],\n",
    "['id', 'weight', 'height', 'age', 'gender', 'income']\n",
    ") # DataFrame 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.rdd) #dataframe 내에 있는데도 타입이 RDD이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.map(\n",
    "    lambda row:(row[\"id\"], sum([c == None for c in row]))\n",
    ").collect() # column별로 row별로 빈 데이터를 찾아준다.\n",
    "#[(해당 row의 id, row 내에 None의 개수)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+------+\n",
      "| id|weight|height| age|gender|income|\n",
      "+---+------+------+----+------+------+\n",
      "|  3|  null|   5.2|null|  null|  null|\n",
      "+---+------+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"id=3\").show() # id가 3인 row를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(weight)|\n",
      "+-------------+\n",
      "|            6|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(f.count(\"weight\")).show() # weight를 가지는 row의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------+------------------------------------------------+------------------------------------------------+\n",
      "|(1 - (count(id) / count(1) AS `idRate`))|(1 - (count(weight) / count(1) AS `weightRate`))|(1 - (count(height) / count(1) AS `heightRate`))|(1 - (count(age) / count(1) AS `ageRate`))|(1 - (count(gender) / count(1) AS `genderRate`))|(1 - (count(income) / count(1) AS `incomeRate`))|\n",
      "+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------+------------------------------------------------+------------------------------------------------+\n",
      "|                                     0.0|                              0.1428571428571429|                                             0.0|                        0.2857142857142857|                              0.1428571428571429|                              0.7142857142857143|\n",
      "+----------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------+------------------------------------------------+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(*[1-f.count(c)/f.count(\"*\").alias(c+\"Rate\") for c in df.columns]).show()\n",
    "# 1 - ( 해당 컬럼 지니는 row 수 /  전체 row 수)\n",
    "# 즉, 모든 데이터에 대해서 각 column에 None을 지니는 비율을 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+\n",
      "| id|weight|height| age|gender|\n",
      "+---+------+------+----+------+\n",
      "|  1| 143.5|   5.6|  28|     M|\n",
      "|  2| 167.2|   5.4|  45|     M|\n",
      "|  3|  null|   5.2|null|  null|\n",
      "|  4| 144.5|   5.9|  33|     M|\n",
      "|  5| 133.2|   5.7|  54|     F|\n",
      "|  6| 124.1|   5.2|null|     F|\n",
      "|  7| 129.2|   5.3|  42|     M|\n",
      "+---+------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([c for c in df.columns if c != \"income\"]).show()\n",
    "# \"income\" Column을 제외한 df를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+------+\n",
      "| id|weight|height| age|gender|income|\n",
      "+---+------+------+----+------+------+\n",
      "|  1| 143.5|   5.6|  28|     M|100000|\n",
      "|  2| 167.2|   5.4|  45|     M|  null|\n",
      "|  3|  null|   5.2|null|  null|  null|\n",
      "|  4| 144.5|   5.9|  33|     M|  null|\n",
      "|  5| 133.2|   5.7|  54|     F|  null|\n",
      "|  6| 124.1|   5.2|null|     F|  null|\n",
      "|  7| 129.2|   5.3|  42|     M| 76000|\n",
      "+---+------+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+------+\n",
      "| id|weight|height|age|gender|income|\n",
      "+---+------+------+---+------+------+\n",
      "|  1| 143.5|   5.6| 28|     M|100000|\n",
      "|  2| 167.2|   5.4| 45|     M|  null|\n",
      "|  4| 144.5|   5.9| 33|     M|  null|\n",
      "|  5| 133.2|   5.7| 54|     F|  null|\n",
      "|  7| 129.2|   5.3| 42|     M| 76000|\n",
      "+---+------+------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(thresh=5).show() # non-null값이 5개 이하인 것을 날려줌\n",
    "# 즉, 6개의 column 중에 null 값이 2개 이상이면 날려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "means = df.agg(\n",
    "*[f.mean(c).alias(c) for c in df.columns if c != \"gender\"]\n",
    ") \n",
    "#gender를 제외한 컬럽에서 mean(평균)을 구해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------+----+-------+\n",
      "| id|            weight|           height| age| income|\n",
      "+---+------------------+-----------------+----+-------+\n",
      "|4.0|140.28333333333333|5.471428571428572|40.4|88000.0|\n",
      "+---+------------------+-----------------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "means.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rdd이던 dataframe이든 immutable하기 때문에 바로 수정하지 못한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpd = means.toPandas() #데이터를 수정하기 위해서 판다스 형식으로 바꿔줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>140.283333</td>\n",
       "      <td>5.471429</td>\n",
       "      <td>40.4</td>\n",
       "      <td>88000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id      weight    height   age   income\n",
       "0  4.0  140.283333  5.471429  40.4  88000.0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpd # DataFrame타입이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd = mpd.to_dict(\"records\")[0] \n",
    "# dict타입으로 해당 DataFrame객체를 바꿔준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 40.399999999999999,\n",
       " 'height': 5.4714285714285724,\n",
       " 'id': 4.0,\n",
       " 'income': 88000.0,\n",
       " 'weight': 140.28333333333333}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpd[\"gender\"] = \"X\" # 새로운 Column을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 40.399999999999999,\n",
       " 'gender': 'X',\n",
       " 'height': 5.4714285714285724,\n",
       " 'id': 4.0,\n",
       " 'income': 88000.0,\n",
       " 'weight': 140.28333333333333}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(mpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------+---+------+------+\n",
      "| id|            weight|height|age|gender|income|\n",
      "+---+------------------+------+---+------+------+\n",
      "|  1|             143.5|   5.6| 28|     M|100000|\n",
      "|  2|             167.2|   5.4| 45|     M| 88000|\n",
      "|  3|140.28333333333333|   5.2| 40|     X| 88000|\n",
      "|  4|             144.5|   5.9| 33|     M| 88000|\n",
      "|  5|             133.2|   5.7| 54|     F| 88000|\n",
      "|  6|             124.1|   5.2| 40|     F| 88000|\n",
      "|  7|             129.2|   5.3| 42|     M| 76000|\n",
      "+---+------------------+------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NULL으로 있던 id=3이 평균값으로 대체되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null값이 많은 컬럼은 날려버린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 방식으로 데이터를 정제할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier들을 날리기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Outlier는 일반적인 범위를 벗어나 있는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 143.5, 5.3, 28, \"M\"),\n",
    "        (2, 154.2, 5.5, 45, \"M\"),\n",
    "        (3, 324.3, 5.1, 99, \"F\"),\n",
    "        (4, 144.5, 5.5, 33, \"M\"),\n",
    "        (5, 133.2, 5.4, 54, \"F\"),\n",
    "        (6, 124.1, 5.1, 21, \"F\"),\n",
    "        (7, 129.2, 5.3, 42, \"M\"),    \n",
    "    ],\n",
    "    ['id', 'weight','height','age']\n",
    ") # DataFrame 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = df.approxQuantile(\"weight\", [0.25, 0.75], 0.05)\n",
    "# df.approxQuantile([Column들], [확률값], 상대오차)\n",
    "# DataFrame객체의  숫자로된 Column들의  대략적인 Quantile를 계산\n",
    "\n",
    "# 0%는 제일 적은 129.2, 100%는 제일 큰 324.3\n",
    "#\"Weight\"컬럼 자료군들 중에 [25%, 75%]에 해당하는 자료를 5%의 상대오차를 허용하여 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[129.2, 154.2]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles # lower bound, upper bound로 맞춘다.\n",
    "# 즉 데이터군의 25% 이하, 75 % 이상의 데이터를 outlier로 간주할 것임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IQR = quantiles[1] - quantiles[0] #  이것은 중간값이 될것이다.\n",
    "# 그리고 이제 bound를 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bounds = [quantiles[0] - 1.5*IQR, quantiles[1] + 1.5 * IQR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[91.69999999999999, 191.7]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"weight\", \"height\", \"age\"]\n",
    "bounds = {}\n",
    "\n",
    "for c in cols:\n",
    "    quantiles = df.approxQuantile(c, [0.25, 0.75], 0.05)\n",
    "    IQR = quantiles[1] - quantiles[0]\n",
    "    bounds[c] = [quantiles[0]-1.5*IQR, quantiles[1]+1.5*IQR]\n",
    "\n",
    "#위에서 구한 것처럼 모든 컬럼에 대해서 lowerbound 25%, upperbound 75%로 놓는다.\n",
    "# 그리고 bound르 생성해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': [-11.0, 93.0],\n",
       " 'height': [4.499999999999999, 6.1000000000000005],\n",
       " 'weight': [91.69999999999999, 191.7]}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|(weight < 91.69999999999999)|\n",
      "+----------------------------+\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "+----------------------------+\n",
      "\n",
      "+----------------+\n",
      "|(weight > 191.7)|\n",
      "+----------------+\n",
      "|           false|\n",
      "|           false|\n",
      "|            true|\n",
      "|           false|\n",
      "|           false|\n",
      "|           false|\n",
      "|           false|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lowerbound보다 낮은 weight를 지녔는가?\n",
    "df.select(df[\"weight\"] < bounds[\"weight\"][0]).show() \n",
    "# upperbound보다 높은 weight를 지녔는가?\n",
    "df.select(df[\"weight\"] > bounds[\"weight\"][1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outlier =  df.select(\n",
    "    *[\"id\"] + [((df[c] < bounds[c][0]) |\n",
    "    (df[c] > bounds[c][1])).alias(c+\"_O\") for c in col]\n",
    ") #  \"id\" 컬럼 데이터 + bound를 넘어가는 outlier에 대한 Boolean값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-----+\n",
      "| id|weight_O|height_O|age_O|\n",
      "+---+--------+--------+-----+\n",
      "|  1|   false|   false|false|\n",
      "|  2|   false|   false|false|\n",
      "|  3|    true|   false| true|\n",
      "|  4|   false|   false|false|\n",
      "|  5|   false|   false|false|\n",
      "|  6|   false|   false|false|\n",
      "|  7|   false|   false|false|\n",
      "+---+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outlier.show()  # true 인 값들이 bound를 넘어가는 outlier들이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joinTable = df.join(outlier, on=\"id\")\n",
    "# \"id\" Column을 기준으로 outlier와 DataFrame객체 데이터를 Join해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+---+--------+--------+-----+\n",
      "| id|weight|height|age| _5|weight_O|height_O|age_O|\n",
      "+---+------+------+---+---+--------+--------+-----+\n",
      "|  7| 129.2|   5.3| 42|  M|   false|   false|false|\n",
      "|  6| 124.1|   5.1| 21|  F|   false|   false|false|\n",
      "|  5| 133.2|   5.4| 54|  F|   false|   false|false|\n",
      "|  1| 143.5|   5.3| 28|  M|   false|   false|false|\n",
      "|  3| 324.3|   5.1| 99|  F|    true|   false| true|\n",
      "|  2| 154.2|   5.5| 45|  M|   false|   false|false|\n",
      "|  4| 144.5|   5.5| 33|  M|   false|   false|false|\n",
      "+---+------+------+---+---+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinTable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n",
      "| id|weight|height|age|\n",
      "+---+------+------+---+\n",
      "|  7| 129.2|   5.3| 42|\n",
      "|  6| 124.1|   5.1| 21|\n",
      "|  5| 133.2|   5.4| 54|\n",
      "|  1| 143.5|   5.3| 28|\n",
      "|  2| 154.2|   5.5| 45|\n",
      "|  4| 144.5|   5.5| 33|\n",
      "+---+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinTable.filter(\"!weight_O\").select(\"id\", \"weight\", \"height\", \"age\").show()\n",
    "# !weight_0를 한 이유는 outlier들을 판별하기 위해..\n",
    "# !를 통해서 해당 weight_0에서 outlier True인 값을 빼주는 것이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
